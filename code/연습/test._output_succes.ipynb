{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3957d7c55b3247389114d89c505f2ead",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from peft import PeftModel, PeftConfig\n",
    "from transformers import AutoModelForCausalLM\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "from langchain import PromptTemplate,  LLMChain, HuggingFacePipeline\n",
    "import gradio as gr\n",
    "from newspaper import Article\n",
    "import re\n",
    "from summa.summarizer import summarize\n",
    "import pandas as pd\n",
    "from kiwipiepy import Kiwi\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from summa.summarizer import summarize\n",
    "config = PeftConfig.from_pretrained(\"re2panda/polyglot_1.3B_plain_1104\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"EleutherAI/polyglot-ko-1.3b\")\n",
    "model = PeftModel.from_pretrained(model, \"re2panda/polyglot_1.3B_plain_1104\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/polyglot-ko-1.3b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "433ce520b7b1412da8c470ad96deb562",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7874\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7874/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def crawling(url): # 엑스포츠\n",
    "    # url = input('url을 입력하세요')\n",
    "    article = Article(url,laguage='ko')\n",
    "    article.download()\n",
    "    article.parse()\n",
    "    title = article.title\n",
    "    text = article.text\n",
    "    text = '.'.join(article.text.split('.')[:-2:])  # 일반 기사로 할시 지움(엑스포츠 기사만 적용)\n",
    "    text = ''.join(text.split('기자) ')[1]) # 일반 기사로 할시 지움(엑스포츠 기사만 적용)\n",
    "    text = re.sub(r\"[^\\uAC00-\\uD7A30-9a-zA-Z\\s,.%()]\", \"\", text)\n",
    "    text_sum = summarize(text,ratio=0.1)\n",
    "    # text_sum = '\\n'.join(summarize(text,word_coutn=200).split('.'))\n",
    "    return title , text , text_sum\n",
    "\n",
    "kiwi = Kiwi()\n",
    "def extract_nouns(text): # 토큰추출\n",
    "        result = kiwi.tokenize(text)\n",
    "        for token in result:\n",
    "            if token.tag in ['NNG', 'NNP']:\n",
    "                yield token.form\n",
    "\n",
    "\n",
    "# print(\"기사내용확인:\", crawling('https://www.xportsnews.com/article/1776390'))\n",
    "\n",
    "def lang_test(url):\n",
    "    title , text , text_sum = crawling(url)\n",
    "    data = {'text': [text]}\n",
    "    df = pd.DataFrame(data)\n",
    "    cv = CountVectorizer(tokenizer=extract_nouns, min_df=1)\n",
    "    dtm = cv.fit_transform(df.text) \n",
    "    word_count = pd.DataFrame({'word': cv.get_feature_names_out(),'count': dtm.sum(axis=0).flat}) #빈도수 만들기\n",
    "    top5_keywords = word_count.sort_values('count', ascending=False).head(5).reset_index(drop=True) # 빈출 빈도수 \n",
    "    pipe = pipeline(\"text-generation\",\n",
    "                model=model,\n",
    "                tokenizer= tokenizer,\n",
    "                torch_dtype=torch.bfloat16,\n",
    "                device_map=\"auto\",\n",
    "                max_new_tokens = 16,\n",
    "                # do_sample=True,\n",
    "                top_k=10,       #무작위성 절제\n",
    "                num_return_sequences=1,\n",
    "                eos_token_id=tokenizer.eos_token_id\n",
    "                )\n",
    "    llm = HuggingFacePipeline(pipeline = pipe, model_kwargs = {'temperature':0})\n",
    "    template = f\"\"\"판별은 입력의 기사제목과 기사내용을 분석하여 해당 기사의 낚시성 기사 또는 정상기사, 낚시기사 유형을 출력합니다.\n",
    "    다음은 기사 제목, 기사 내용를 제공하는 입력과 짝을 이루는 판별 작업을 명령하는 지침입니다. 요청을 적절하게 완료하는 응답을 작성합니다.\n",
    "    ### 명령:\n",
    "    주어진 기사를 읽고 낚시성 기사 유무를 판별하라.\n",
    "    ### 입력:\n",
    "    기사 제목 : {title} , 기사 내용 :{text}\n",
    "    ### 판별:\n",
    "    해당기사는\"\"\" \n",
    "    \n",
    "    prompt = PromptTemplate(template=template, input_variables=['title','text'])\n",
    "    llm_chain = LLMChain(prompt=prompt, llm=llm)\n",
    "    answer = llm_chain.predict() \n",
    "    # answer = llm_chain.predict()\n",
    "    # if '낚시성기사입니다' in answer:\n",
    "    #     answer\n",
    "    # else:\n",
    "    #     answer = llm_chain.predict() + f'\\n\\n <본문 요약> \\n {text_sum}'  \n",
    "    return answer , text_sum , top5_keywords\n",
    "\n",
    "# lang_test('https://www.xportsnews.com/article/1776390')\n",
    "\n",
    "iface = gr.Interface(\n",
    "    fn=lang_test,\n",
    "    inputs=gr.Textbox(lines=2, placeholder=\"URL을 입력해주세요\"),\n",
    "    outputs=[gr.Textbox(lines=4,label='낚시성 판별 결과'),gr.Textbox(label=\"요약 정보\"),gr.Textbox(label=\"빈출 빈도 수\")],\n",
    "    title = \"<center><img src='C:/Users/administ/Desktop/cj해커톤/image.png' width=1000 height=200></center>\",\n",
    "    theme='soft',\n",
    "    description=\n",
    "                \"<center><div style='font-size: 20px; margin-bottom: 30px;'><strong>진실을 찾아가는 PressPulse, 언론의 건강상태를 체크하세요.</strong></div></center>\"\n",
    "                \"<div style='font-size: 15px; margin-bottom: 10px;'><strong>기사 URL을 입력하면 낚시 여부를 판별해주는 시스템 입니다.</strong></div>\"\n",
    "                \"<ol style='font-size: 15px;'>\"\n",
    "                \"<li><strong>기사 URL을 입력하시고 'submit' 버튼을 눌러주세요<strong></li>\"\n",
    "                \"<li><strong>낚시기사 판별 여부, 낚시 유형을 확인 가능합니다 (낚시기사가 아닐 시 유형이 나오지 않습니다)<strong></li>\"\n",
    "                \"</ol>\",\n",
    "    examples=[[\"https://www.xportsnews.com/article/1742486 \"], \n",
    "              ['https://www.xportsnews.com/article/1742486'],\n",
    "              ['https://www.xportsnews.com/article/1776390']]\n",
    ")\n",
    "iface.launch(inline=True)\n",
    "# iface.launch(share=True,inline=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 정상 기사"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7862\n",
      "Running on public URL: https://6a474fc5e2e2824aa8.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://6a474fc5e2e2824aa8.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "url을 입력하세요"
     ]
    }
   ],
   "source": [
    "from summa.summarizer import summarize\n",
    "\n",
    "def crawling(url):\n",
    "    url = input('url을 입력하세요')\n",
    "    article = Article(url,laguage='ko')\n",
    "    article.download()\n",
    "    article.parse()\n",
    "    title = article.title\n",
    "    text = article.text\n",
    "    text = re.sub(r\"[^\\uAC00-\\uD7A30-9a-zA-Z\\s,.%()]\", \"\", text)\n",
    "    text_sum = '\\n'.join(summarize(text).split('.'))\n",
    "    return title , text , text_sum\n",
    "\n",
    "# print(\"기사내용확인:\", crawling('https://www.xportsnews.com/article/1776390'))\n",
    "\n",
    "def lang_test(url):\n",
    "    title , text , text_sum = crawling(url)\n",
    "    pipe = pipeline(\"text-generation\",\n",
    "                model=model,\n",
    "                tokenizer= tokenizer,\n",
    "                torch_dtype=torch.bfloat16,\n",
    "                device_map=\"auto\",\n",
    "                max_new_tokens = 16,\n",
    "                # do_sample=True,\n",
    "                top_k=10,       #무작위성 절제\n",
    "                num_return_sequences=1,\n",
    "                eos_token_id=tokenizer.eos_token_id\n",
    "                )\n",
    "    llm = HuggingFacePipeline(pipeline = pipe, model_kwargs = {'temperature':0})\n",
    "    template = f\"\"\"판별은 입력의 기사제목과 기사내용을 분석하여 해당 기사의 낚시성 기사 또는 정상기사, 낚시기사 유형을 출력합니다.\n",
    "    다음은 기사 제목, 기사 내용를 제공하는 입력과 짝을 이루는 판별 작업을 명령하는 지침입니다. 요청을 적절하게 완료하는 응답을 작성합니다.\n",
    "    ### 명령:\n",
    "    주어진 기사를 읽고 낚시성 기사 유무를 판별하라.\n",
    "    ### 입력:\n",
    "    기사 제목 : {title} , 기사 내용 :{text}\n",
    "    ### 판별:\n",
    "    해당기사는\"\"\" \n",
    "    \n",
    "    prompt = PromptTemplate(template=template, input_variables=['title','text'])\n",
    "    llm_chain = LLMChain(prompt=prompt, llm=llm)\n",
    "    answer = llm_chain.predict() + f'\\n\\n <본문 요약> \\n {text_sum}'\n",
    "    # answer = llm_chain.predict()\n",
    "    # if '낚시성기사입니다' in answer:\n",
    "    #     answer\n",
    "    # else:\n",
    "    #     answer = llm_chain.predict() + f'\\n\\n <본문 요약> \\n {text_sum}'  \n",
    "    return answer\n",
    "\n",
    "# lang_test('https://www.xportsnews.com/article/1776390')\n",
    "\n",
    "iface = gr.Interface(\n",
    "    fn=lang_test,\n",
    "    inputs=gr.Textbox(lines=4, placeholder=\"URL을 입력해주세요\"),\n",
    "    outputs=gr.Textbox(lines=4),\n",
    "    title=\"낭만어부 판별 시스템\",\n",
    "    theme='soft',\n",
    "    description=\"<div style='font-size: 15px; margin-bottom: 10px;'><strong>기사 URL을 입력하면 낚시 여부를 판별해주는 시스템 입니다.</strong></div>\"\n",
    "               \"<ol style='font-size: 15px;'>\"\n",
    "               \"<li><strong>기사 URL을 입력하시고 'submit' 버튼을 눌러주세요<strong></li>\"\n",
    "               \"<li><strong>낚시기사 판별 여부, 낚시 유형을 확인 가능합니다 (낚시기사가 아닐 시 유형이 나오지 않습니다)<strong></li>\"\n",
    "               \"</ol>\",\n",
    "    examples=[[\"https://www.xportsnews.com/article/1742486 \"], \n",
    "              ['https://www.xportsnews.com/article/1742486'],\n",
    "              ['https://www.xportsnews.com/article/1776390']]\n",
    ")\n",
    "\n",
    "iface.launch(share=True,inline=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
